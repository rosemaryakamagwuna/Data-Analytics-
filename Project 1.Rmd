---
title: "Stat557_project1_RosemaryAkamagwuna"
output: html_document
date: "2023-09-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
```{r}
## Read the data
xTrain=read.csv("ecoli_xTrain.csv", header=F)
yTrain=read.csv("ecoli_yTrain.csv", header=F)
xTest=read.csv("ecoli_xTest.csv", header=F)
yTest=read.csv("ecoli_yTest.csv", header=F)
```

```{r}
#install.packages('caret')
library(caret)
library(pROC)

```



```{r}
print(xTrain)
print(xTest)
print(yTrain)
print(yTest)

```

```{r}
unique(yTrain)
yTrain_ <- yTrain$V1
yTest_ <- yTest$V1 
```
  



```{r}
#### Part 1 ####
logProd <- function(x){
  if (length(x) == 0) {
    return(NA)  # Return NA for an empty vector
  }
 
  result = sum(x)
  return(result) 
}

# Calculate log(∏p) using the function
log_result <- logProd(runif(n=10, min=-12, max=0))

# Print the result

print(log_result)

```

```{r}
#### Part 2 ####
logSum <- function(x) {
  if (length(x) == 0) {
    return(NA)  # Return NA for an empty vector
  }
  
  n <- length(x)
  if (n == 1) {
    return(log(x[1]))  # Logarithm of a single value is the value itself
  }
  
  # Initialize the result with the first element
  max_x <- max(x)
  
  return(max_x + log(sum(exp(x - max_x))))
}

# Calculate log(∏p) using the function
log_result_sum <- logSum(runif(n=10, min=-12, max=0))

# Print the result

print(log_result_sum)


```


</br>
#Question 2

```{r}
###Part 1
prior <- function(yTrain) {
  unique_classes <- unique(yTrain)
  n <- length(yTrain)
  prior_probabilities <- numeric(length(unique_classes))

  for (i in 1:length(unique_classes)) {
    class_i <- unique_classes[i]
    prior_probabilities[i] <- sum(yTrain == class_i) / n
  }


  names(prior_probabilities) <- unique_classes
  
  return(prior_probabilities)
}


prior_ <- prior(yTrain_)
# Print the prior probabilities
print(prior_)



```

```{r}
###Part 2

likelihood <- function(xTrain, yTrain) {
  unique_classes <- unique(yTrain)
  n <- nrow(xTrain)  # Number of samples
  m <- ncol(xTrain)  # Number of features
  c <- length(unique_classes)  # Number of unique classes

  # Initialize matrices for conditional means (M) and variances (V)
  M <- matrix(0, nrow=m, ncol=c)
  V <- matrix(0, nrow=m, ncol=c)
  
 # Create vectors to store mappings of means and variances to repetitions of classes
  class_means <- rep(0, n)
  class_variances <- rep(0, n)

  # Calculate conditional means and variances for each feature and class
  for (j in 1:c) {
    class_j <- unique_classes[j]
    x_j <- xTrain[yTrain == class_j, ]  # Subset of features for class j
    n_j <- nrow(x_j)  # Number of samples in class j

    for (i in 1:m) {
      feature_i <- x_j[, i]
      mean_i <- mean(feature_i)
      var_i <- var(feature_i)

      M[i, j] <- mean_i
      V[i, j] <- var_i
      
      # Map means and variances to repetitions of classes
      class_indices <- which(yTrain == class_j)
      class_means[class_indices] <- mean_i
      class_variances[class_indices] <- var_i
    }
  }

   # Return matrices M and V along with class_means and class_variances
  return(list(M = M, V = V, ClassMeans = class_means, ClassVariances = class_variances))

}

likelihood_matrices <- likelihood(xTrain, yTrain_)
M_matrix <- likelihood_matrices$M
V_matrix <- likelihood_matrices$V

#class_means_vector <- likelihood_matrices$ClassMeans
#class_variances_vector <- likelihood_matrices$ClassVariances
#print(class_means_vector)
#print(class_variances_vector)

```


```{r}

# Define the naiveBayesClassify function
naiveBayesClassify <- function(xTest, M, V, p) {
  n <- nrow(xTest)  # Number of rows in xTest
  c <- ncol(M)      # Number of unique classes (same as in M and V)
  
  # Initialize the vector t to store predicted class values
#  t <- vector("integer", length = n)
  
  # Calculate the log likelihood for each class
  log_posteriors <- matrix(0, nrow=n, ncol=c)
  
  for (i in 1:n) {
    row_i <- as.numeric(xTest[i, ])  # Get the i-th row of xTest
    
    # Calculate the log likelihood for each class
    for (j in 1:c) {
      log_likelihood_i_j <- logProd(dnorm(row_i, mean=M[, j], sd=sqrt(V[, j]), log=TRUE))
      log_posteriors[i, j] <- logSum(c(log_likelihood_i_j, log(p[j])))
    }
  }
  
  # Choose the class with the maximum log-posterior as the predicted class
    t <- apply(log_posteriors, 1, which.max)
  
  return(t)
}

y_pred_GNB <- naiveBayesClassify(xTest, M_matrix, V_matrix, prior_)

result_GNB <- confusionMatrix(as.factor(y_pred_GNB), as.factor(yTest$V1))

result_GNB$overall['Accuracy']
result_GNB$byClass[1,'Precision']
result_GNB$byClass[1,'Recall']
result_GNB$byClass[5,'Precision']
result_GNB$byClass[5,'Recall']
```

</br>
#Question 3
```{r}
## Read the data
xTrain = read.csv("ecoli_new.xTrain.csv", header=F)
yTrain = read.csv("ecoli_new.yTrain.csv", header=F)
xTest = read.csv("ecoli_new.xTest.csv", header=F)
yTest = read.csv("ecoli_new.yTest.csv", header=F)

```

```{r}
sigmoidProb <- function(y, x, w){

  posterior_prob <- 1 / (1 + exp(sum(x * w)))
  
  if (y == 1){
    posterior_prob <- 1 - posterior_prob
  }
  
  return(posterior_prob)
}

```

```{r}
logisticRegressionWeights <- function(xTrain, yTrain, w0, nIter){
  step_size <- 0.1
  epsilon <- 0.1
  
  w0 = as.vector(w0)
  n <- nrow(xTrain)
  f <- ncol(xTrain)
  
  updates <- matrix(0, nrow=n, ncol=f)
  
  is_converged <- FALSE
  
  for (iter in 1:nIter){
    for (i in 1:n){
      y <- as.numeric(yTrain[i, 1])
      prob <- sigmoidProb(y, as.numeric(xTrain[i, ]), w0)
      
      if (y == 0){
        prob <- 1 - prob
      }
      
      for (j in 1:f){
        updates[i, j] <- as.numeric(xTrain[i, j])*(y - prob)
      }
    }
    
    w_new <- w0 + step_size*apply(updates, 2, sum)
    
    w_percent_delta <- max(abs((w_new - w0) / w0))
    
    if (w_percent_delta < epsilon){
      is_converged <- TRUE
      print(paste("Algorithm coverged after ", iter, "iterations"))
      break
    }
    
    w0 <- w_new
    
    
  }
  
  if (!is_converged){
  print(paste("Algorithm did not coverged. Max oercentage delta is: ", 100*w_percent_delta))
  }
  
  return(matrix(w_new))
}
```


</br>

```{r}
logisticRegressionClassify <- function(xTest, w){
  threshold <- 0.5
  
  n <- nrow(xTest)
  
  prob_1 <- rep(0, n)
  
  for (i in 1:n){
    prob_1[i] <- sigmoidProb(1, as.numeric(xTest[i, ]), w)
  }
  
  pred <- rep(0, n)
  
  pred[prob_1 >= threshold] <- 1
  
  return(pred)
}
```

```{r}
w0 <- matrix(1, nrow=6, ncol=1)

w <- logisticRegressionWeights(xTrain, yTrain, w0, 100)

ypred_LR <- logisticRegressionClassify(xTest, w)

result_LR <- confusionMatrix(as.factor(ypred_LR), as.factor(yTest$V1))

result_LR$overall['Accuracy']
result_LR$byClass['Precision']
result_LR$byClass['Recall']


```

```{r}
eval_file <- file("evaluation.txt")
writeLines(c(
  paste("Reports for Gaussian Naive Bayes Model"),
  paste("Accuracy is:", as.character(round(result_GNB$overall['Accuracy'], 3))),
  paste("Precision for Class 1 is:", as.character(round(result_GNB$byClass[1,'Precision'], 3))),
   paste("Recall for Class 1 is:", as.character(round(result_GNB$byClass[1,'Recall'], 3))),
   paste("Precision for Class 5 is:", as.character(round(result_GNB$byClass[5,'Precision'], 3))),
   paste("Recall for Class 5 is:", as.character(round(result_GNB$byClass[5,'Recall'], 3))), 
  paste(""),
  paste("Reports for Logistics Regression Model"),
  paste("Accuracy is:", as.character(round(result_LR$overall['Accuracy'], 3))),
  paste("Precision for Class 1 is:", as.character(round(result_LR$byClass['Precision'], 3))),
   paste("Recall for Class 1 is:", as.character(round(result_LR$byClass['Recall'], 3)))
)
  ,  
  eval_file)
close(eval_file)
```

